{
  # -- Full license text available in LICENSE.md
  "license" {
    "accept": true
  }

  "input": {
    # -- Kinesis stream for the source of collector payloads
    "streamName": "snowplow-collector-payloads"

    # -- Name to use for the KCL DynamoDB table
    "appName": "snowplow-enrich"

    # -- From where the loader should start consuming if this is the first time it is run
    # -- On subsequent runs, it will always resume from where it last checkpointed
    "initialPosition": {
      # -- Options are `TRIM_HORIZON` for the oldest available events, `LATEST` for latest events,
      # -- or `AT_TIMESTAMP` to start consuming from events written at a particular time
      "type": "TRIM_HORIZON"

      # -- Only required if `initialPosition.type` is AT_TIMESTAMP
      # "timestamp": "2023-01-01T00:00:00Z"
    }

    # -- How the underlying Kinesis client should fetch events from the stream
    "retrievalMode": {
      # -- Options are "Polling" for the client to poll Kinesis for more events when needed
      # -- or "FanOut" to enabled Kinesis's Enhanced Fan Out feature using HTTP/2
      "type": "Polling"

      # -- Only used if retrieval mode is type Polling. How many events the client may fetch in a single poll
      "maxRecords": 1000
    }

    # -- Name of this KCL worker used in the DynamoDB lease table
    "workerIdentifier": ${HOSTNAME}

    # -- Duration of shard leases. KCL workers must periodically refresh leases in the DynamoDB table before this duration expires
    "leaseDuration": "10 seconds"

    # -- Controls how to pick the max number of leases to steal at one time
    # -- E.g. If there are 4 available processors, and maxLeasesToStealAtOneTimeFactor = 2.0, then allow the KCL to steal up to 8 leases.
    # -- Allows bigger instances to more quickly acquire the shard-leases they need to combat latency
    "maxLeasesToStealAtOneTimeFactor": 2.0

    # -- How to backoff and retry in case of DynamoDB provisioned throughput limits
    "checkpointThrottledBackoffPolicy": {
      "minBackoff": "100 millis"
      "maxBackoff": "1 second"
    }

    ## -- How frequently to checkpoint our progress to the DynamoDB table. By increasing this value,
    ## -- we can decrease the write-throughput requirements of the DynamoDB table
    debounceCheckpoints: "10 seconds"
  }

  "output": {

    "enriched": {
      # -- Output kinesis stream for enriched events
      "streamName": "snowplow-enriched"

      # -- How to retry sending enriched events if we exceed the kinesis write throughput limits
      "throttledBackoffPolicy": {
        "minBackoff": "100 milliseconds"
        "maxBackoff": "1 second"
      }

      # -- Maximum allowed to records we are allowed to send to Kinesis in 1 PutRecords request
      "recordLimit": 500

      # -- Maximum allowed to bytes we are allowed to send to Kinesis in 1 PutRecords request
      "byteLimit": 5242880

      # -- Optional. Partition key for the enriched events in Kinesis
      # If not set, a random UUID is used
      "partitionKey": "user_id"
    }

    "failed": {
      # -- Output kinesis stream for failed events that could not be validated or enriched (TSV)
      "streamName": "snowplow-failed"

      # -- How to retry sending failed events if we exceed the kinesis write throughput limits
      "throttledBackoffPolicy": {
        "minBackoff": "100 milliseconds"
        "maxBackoff": "1 second"
      }

      # -- Maximum allowed to records we are allowed to send to Kinesis in 1 PutRecords request
      "recordLimit": 500

      # -- Maximum allowed to bytes we are allowed to send to Kinesis in 1 PutRecords request
      "byteLimit": 5242880
    }

    "bad": {
      # -- Output kinesis stream for bad rows (JSON)
      "streamName": "snowplow-bad"

      # -- How to retry sending bad rows if we exceed the kinesis write throughput limits
      "throttledBackoffPolicy": {
        "minBackoff": "100 milliseconds"
        "maxBackoff": "1 second"
      }

      # -- Maximum allowed to records we are allowed to send to Kinesis in 1 PutRecords request
      "recordLimit": 500

      # -- Maximum allowed to bytes we are allowed to send to Kinesis in 1 PutRecords request
      "byteLimit": 5242880
    }
  }

  # -- Controls how the app splits the workload into concurrent batches which can be run in parallel
  # -- E.g. If there are 4 available processors, and cpuParallelismFactor = 0.75, then we process 3 batches concurrently.
  # -- Adjusting this value can cause the app to use more or less of the available CPU.
  # -- Steps with parallelism: parsing input bytes, validating and enriching, serializing output records
  "cpuParallelismFraction": 0.75

  "monitoring": {

    "metrics": {
      # -- Send runtime metrics to a statsd server
      "statsd": {
        "hostname": "127.0.0.1"
        "port": 8125

        # -- Map of key/value pairs to be send along with the metric
        "tags": {
          "env": "prod"
        }

        # -- How often to report metrics
        "period": "1 minute"

        # -- Prefix used for the metric name when sending to statsd
        "prefix": "snowplow.enrich"
      }
    }

    # -- Report unexpected runtime exceptions to Sentry
    "sentry": {
      "dsn": "https://public@sentry.example.com/1"

      # -- Map of key/value pairs to be included as tags
      "tags": {
        "myTag": "xyz"
      }
    }

    # -- Open a HTTP server that returns OK only if the app is healthy
    "healthProbe": {
      "port": 8000

      # -- Health probe becomes unhealthy if any received event is still not fully processed before
      # -- this cutoff time
      "unhealthyLatency": "2 minutes"
    }
  }

  # -- Period after which enrich assets should be checked for updates
  "assetsUpdatePeriod": "7 days"

  # -- Configuration of internal http client used for iglu resolver, alerts, telemetry and metadata
  "http": {
    "client": {
      "maxConnectionsPerServer": 4
    }
  }

  "validation" : {

    # When set to true, events with fields too long still get emitted as valid enriched events
    # WARNING: this feature flag will be removed in a future version
    "acceptInvalid": false

    # Override default maximum atomic fields (strings) length
    # Map-like structure with keys being field names and values being their max allowed length
    "atomicFieldsLimits": {
        "app_id": 5
        "mkt_clickid": 100000
        # ...and any other 'atomic' field with custom limit
    }

    # Maximum allowed depth for the JSON entities in the events. The default value is 40
    # Event will be sent to bad row stream if it contains JSON entity with a depth that exceeds this value
    "maxJsonDepth": 50

    # Default is true.
    # If it is set to true, Enrich will exit with error if JS enrichment
    # script is invalid.
    # If it is set to false, Enrich will continue to run if JS enrichment
    # script is invalid but every event will end up as bad row.
    "exitOnJsCompileError": true
  }

  # Optional. Configure telemetry
  "telemetry": {

    # Set to true to disable telemetry
    "disable": false

    # Interval for the heartbeat event
    "interval": 15 minutes

    # HTTP method used to send the heartbeat event
    "method": POST

    # URI of the collector receiving the heartbeat event
    "collectorUri": "https://collector-g.snowplowanalytics.com"

    # Identifier intended to tie events together across modules,
    # infrastructure and apps when used consistently
    "userProvidedId": my_pipeline

    # ID automatically generated upon running a modules deployment script
    # Intended to identify each independent module, and the infrastructure it controls
    "autoGeneratedId": hfy67e5ydhtrd

    # Unique identifier for the VM instance
    # Unique for each instance of the app running within a module
    "instanceId": 665bhft5u6udjf

    # Name of the terraform module that deployed the app
    "moduleName": enrich-kinesis-ce

    # Version of the terraform module that deployed the app
    "moduleVersion": 1.0.0
  }

  # Optional. Whether to export metadata using a webhook URL
  # Follows iglu-webhook protocol
  "metadata": {
    "endpoint": "https://my_pipeline.my_domain.com/iglu"
    "interval": 5 minutes
    "organizationId": "c5f3a09f-75f8-4309-bec5-fea560f78455"
    "pipelineId": "75a13583-5c99-40e3-81fc-541084dfc784"
    # Maximum body size for the payloads sent to the endpoint
    "maxBodySize": 150000
  }
}
