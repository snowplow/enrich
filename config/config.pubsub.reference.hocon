{
  # -- Full license text available in LICENSE.md
  "license" {
    "accept": true
  }

  "input": {
    # -- pubsub subscription for the source of collector payloads
    "subscription": "projects/myproject/subscriptions/snowplow-collector-payloads"

    # -- Pubsub ack deadlines are extended for this duration when needed.
    "durationPerAckExtension": "15 seconds"

    # -- Controls when ack deadlines are re-extended, for a message that is close to exceeding its ack deadline.
    # -- For example, if `durationPerAckExtension` is `15 seconds` and `minRemainingAckDeadline` is `0.1` then the Source
    # -- will wait until there is `1.5 seconds` left of the remaining deadline, before re-extending the message deadline.
    "minRemainingAckDeadline": 0.1

    # -- Configures how transient GRPC failures are retried
    "retries": {
      "transientErrors": {
        # -- Backoff delay for follow-up attempts
        "delay": "100 millis"
        # -- Max number of attempts, after which Enrich will crash and exit
        "attempts": 10
      }
    }
  }

  "output": {

    "good": {
      # -- Output pubsub topic for enriched events
      "topic": "projects/myproject/topics/snowplow-enriched"

      # Optional. Enriched event fields to add as PubSub message attributes.
      "attributes": [ "app_id" ]

      # -- Enriched events are sent to pubsub in batches not exceeding this size.
      "batchSize": 100
      # -- Enriched events are sent to pubsub in batches not exceeding this size number of bytes
      "requestByteThreshold": 1000000

      # -- Configures how transient GRPC failures are retried
      "retries": {
        "transientErrors": {
          # -- Backoff delay for follow-up attempts
          "delay": "100 millis"
          # -- Max number of attempts, after which Enrich will crash and exit
          "attempts": 10
        }
      }
    }

    "failed": {
      # -- Output pubsub topic for failed events that could not be validated or enriched (TSV)
      "topic": "projects/myproject/topics/snowplow-failed"

      # -- Failed events are sent to pubsub in batches not exceeding this size.
      "batchSize": 100
      # -- Failed events are sent to pubsub in batches not exceeding this size number of bytes
      "requestByteThreshold": 1000000

      # -- Configures how transient GRPC failures are retried
      "retries": {
        "transientErrors": {
          # -- Backoff delay for follow-up attempts
          "delay": "100 millis"
          # -- Max number of attempts, after which Enrich will crash and exit
          "attempts": 10
        }
      }
    }

    "bad": {
      # -- Output pubsub topic for bad rows (JSON)
      "topic": "projects/myproject/topics/snowplow-bad"

      # -- Bad events are sent to pubsub in batches not exceeding this size.
      "batchSize": 100
      # -- Bad events are sent to pubsub in batches not exceeding this size number of bytes
      "requestByteThreshold": 1000000

      # -- Configures how transient GRPC failures are retried
      "retries": {
        "transientErrors": {
          # -- Backoff delay for follow-up attempts
          "delay": "100 millis"
          # -- Max number of attempts, after which Enrich will crash and exit
          "attempts": 10
        }
      }
    }
  }

  # -- Controls how the app splits the workload into concurrent batches which can be run in parallel
  # -- E.g. If there are 4 available processors, and cpuParallelismFactor = 0.75, then we process 3 batches concurrently.
  # -- Adjusting this value can cause the app to use more or less of the available CPU.
  # -- Steps with parallelism: validating and enriching
  "cpuParallelismFraction": 1

  # -- Controls number of sink job that can be run in parallel
  # -- E.g. If there are 4 available processors, and sinkParallelismFraction = 2, then we run 8 sink job concurrently.
  # -- Adjusting this value can cause the app to use more or less of the available CPU.
  "sinkParallelismFraction": 2

  "monitoring": {

    "metrics": {
      # -- Send runtime metrics to a statsd server
      "statsd": {
        "hostname": "127.0.0.1"
        "port": 8125

        # -- Map of key/value pairs to be send along with the metric
        "tags": {
          "env": "prod"
        }

        # -- How often to report metrics
        "period": "1 minute"

        # -- Prefix used for the metric name when sending to statsd
        "prefix": "snowplow.enrich"
      }
    }

    # -- Report unexpected runtime exceptions to Sentry
    "sentry": {
      "dsn": "https://public@sentry.example.com/1"

      # -- Map of key/value pairs to be included as tags
      "tags": {
        "myTag": "xyz"
      }
    }

    # -- Open a HTTP server that returns OK only if the app is healthy
    "healthProbe": {
      "port": 8000

      # -- Health probe becomes unhealthy if any received event is still not fully processed before
      # -- this cutoff time
      "unhealthyLatency": "2 minutes"
    }
  }

  # -- Period after which enrich assets should be checked for updates
  "assetsUpdatePeriod": "7 days"

  "validation" : {

    # -- When set to true, events with fields too long still get emitted as valid enriched events
    # -- WARNING: this feature flag will be removed in a future version
    "acceptInvalid": false

    # -- Override default maximum atomic fields (strings) length
    # -- Map-like structure with keys being field names and values being their max allowed length
    "atomicFieldsLimits": {
        "app_id": 5
        "mkt_clickid": 100000
        # ...and any other 'atomic' field with custom limit
    }

    # -- Maximum allowed depth for the JSON entities in the events. The default value is 40
    # -- Event will be sent to bad row stream if it contains JSON entity with a depth that exceeds this value
    "maxJsonDepth": 50

    # -- Default is true.
    # -- If it is set to true, Enrich will exit with error if JS enrichment
    # -- script is invalid.
    # -- If it is set to false, Enrich will continue to run if JS enrichment
    # -- script is invalid but every event will end up as bad row.
    "exitOnJsCompileError": true
  }

  # -- Optional. Configure telemetry
  "telemetry": {

    # -- Set to true to disable telemetry
    "disable": false

    # -- Interval for the heartbeat event
    "interval": 15 minutes

    # -- URI of the collector receiving the heartbeat event
    "collectorUri": "https://collector-g.snowplowanalytics.com"

    # -- Identifier intended to tie events together across modules,
    # -- infrastructure and apps when used consistently
    "userProvidedId": my_pipeline

    # -- ID automatically generated upon running a modules deployment script
    # -- Intended to identify each independent module, and the infrastructure it controls
    "autoGeneratedId": hfy67e5ydhtrd

    # -- Unique identifier for the VM instance
    # -- Unique for each instance of the app running within a module
    "instanceId": 665bhft5u6udjf

    # -- Name of the terraform module that deployed the app
    "moduleName": enrich-kinesis-ce

    # -- Version of the terraform module that deployed the app
    "moduleVersion": 1.0.0
  }

  # -- Optional. Whether to export metadata using a webhook URL
  # -- Follows iglu-webhook protocol
  "metadata": {
    "endpoint": "https://my_pipeline.my_domain.com/iglu"
    "interval": 5 minutes
    "organizationId": "c5f3a09f-75f8-4309-bec5-fea560f78455"
    "pipelineId": "75a13583-5c99-40e3-81fc-541084dfc784"

    # -- Maximum body size for the payloads sent to the endpoint
    "maxBodySize": 150000
  }

  # -- Enrich events via an external identity service
  "identity": {
    "endpoint": "http://identity-api"
    "username": "snowplow"
    "password": "sn0wp10w"

    # -- Controls the number of requests that can be made in parallel to the identity service.
    # -- E.g. If there are 4 available processors and concurrencyFactor = 0.75 then 3 requests can be made concurrently.
    # -- Adjusting this value can cause the app to use more or less of the available CPU and memory.
    "concurrencyFactor": 0.75

    # -- Configures how HTTP requests are retried in case of failure
    "retries": {
      # -- Backoff delay for follow-up attempts
      "delay": "100 millis"
      # -- Max number of attempts, after which Enrich will crash and exit
      "attempts": 3
    }

    # -- Optional. Configure custom identifiers to extract from events
    "customIdentifiers": {
      # -- List of identifier configurations
      "identifiers": [
        {
          # -- Name label for this identifier (e.g., "user_id", "custom_id")
          "name": "user_id"

          # -- Field to extract the identifier value from
          "field": {
            # -- Type of field: "atomic", "event", or "entity"
            "type": "atomic"
            # -- Name of the atomic field
            "name": "user_id"
          }

          # -- Priority for this identifier (lower number = higher priority)
          "priority": 1

          # -- Whether this identifier should be unique within a profile
          "unique": true
        }
        {
          # -- Example: Custom identifier from entity
          "name": "custom_user_id"

          "field": {
            "type": "entity"
            # -- Schema vendor
            "vendor": "com.example"
            # -- Schema name
            "name": "user_context"
            # -- Schema major version
            "major_version": 1
            # -- JSONPath to the field within the entity data
            "path": "$.userId"
            # -- Optional. Index of the entity when multiple entities of the same type exist (0-based, defaults to 0)
            "index": 0
          }

          "priority": 2
          "unique": false
        }
        {
          # -- Example: Identifier from unstructured event
          "name": "event_user_id"

          "field": {
            "type": "event"
            "vendor": "com.example"
            "name": "login_event"
            "major_version": 1
            "path": "$.userId"
          }

          "priority": 3
          "unique": false
        }
      ]

      # -- Optional. Event filtering configuration
      "filters": {
        # -- Logic to apply: "all" (AND) or "any" (OR)
        "logic": "all"

        # -- List of filter rules
        "rules": [
          {
            # -- Field to evaluate
            "field": {
              "type": "atomic"
              "name": "app_id"
            }

            # -- Operator: "in" or "nin" (not in)
            "operator": "in"

            # -- List of values to match
            "values": ["production_app"]
          }
          {
            "field": {
              "type": "atomic"
              "name": "user_id"
            }

            "operator": "nin"

            # -- Filter out events with empty user_id
            "values": [""]
          }
        ]
      }
    }
  }

  # -- Configures behavior of decompresses incoming data
  "decompression": {

    # -- A cutoff used when incrementally adding events to a batch. The batch is emitted immediately
    # -- after this cutoff size is reached. This config parameter is needed to protect the app's memory.
    # -- Bear in mind a 1MB compressed message could become HUGE after decompression.
    # -- This is also used to batch together events from consecutive compressed records inside a chunk.
    "maxBytesInBatch": 5242880

    # -- Each individual collector payload should not exceed this size after decompression.
    # -- Payloads exceeding this size become a size violation type of bad row.
    # -- This config parameter is needed to protect the app's memory.
    "maxBytesSinglePayload": 10000000
  }
}
